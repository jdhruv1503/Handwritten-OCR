{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.special import softmax\n",
        "from joblib import Parallel, delayed\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Sg3kvsSOcwRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DKL-s_oc4Cr",
        "outputId": "f0aaf7c5-3024-41d7-c476-a77a34971b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_8SepSMcbDY"
      },
      "outputs": [],
      "source": [
        "def load_images(folder_path, image_size=(28, 28), val_split=0.2, random_state=42):\n",
        "    print(f\"Loading images from {folder_path}...\")\n",
        "    data, labels = [], []\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # Get the list of folder names\n",
        "    folder_names = sorted(os.listdir(folder_path))\n",
        "    label_encoder.fit(folder_names)  # Fit the encoder with folder names as labels\n",
        "\n",
        "    for label_name in folder_names:\n",
        "        label_folder = os.path.join(folder_path, label_name)\n",
        "        if not os.path.isdir(label_folder):\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(label_folder):\n",
        "            img_path = os.path.join(label_folder, filename)\n",
        "            if not img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Load and preprocess the image\n",
        "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "                img = img.resize(image_size)  # Resize to the target size\n",
        "                img_array = np.array(img).flatten() / 255.0  # Flatten and normalize\n",
        "                data.append(img_array)\n",
        "                labels.append(label_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = label_encoder.transform(labels)  # Encode labels as integers\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        data, labels, test_size=val_split, random_state=random_state, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Loaded {len(data)} images from {len(folder_names)} classes.\")\n",
        "    print(f\"Train set size: {len(X_train)}, Validation set size: {len(X_val)}\")\n",
        "\n",
        "    return X_train, X_val, y_train, y_val, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "        self.quantiles = 20\n",
        "\n",
        "    class Node:\n",
        "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "            self.feature = feature\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "            self.value = value\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        if depth >= self.max_depth or n_samples < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(y)\n",
        "            return self.Node(value=leaf_value)\n",
        "\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            leaf_value = self._calculate_leaf_value(y)\n",
        "            return self.Node(value=leaf_value)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
        "        left_child = self._build_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
        "        right_child = self._build_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
        "\n",
        "        return self.Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples <= 1:\n",
        "            return None, None\n",
        "\n",
        "        best_mse = float('inf')\n",
        "        best_feature, best_threshold = None, None\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            thresholds, mse_values = self._find_thresholds_and_mse(X[:, feature], y)\n",
        "\n",
        "            # Only proceed if there are valid MSE values and thresholds\n",
        "            if mse_values:\n",
        "                min_mse_index = np.argmin(mse_values)\n",
        "                if mse_values[min_mse_index] < best_mse:\n",
        "                    best_mse = mse_values[min_mse_index]\n",
        "                    best_feature = feature\n",
        "                    best_threshold = thresholds[min_mse_index]\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _find_thresholds_and_mse(self, feature_values, y):\n",
        "        sorted_indices = np.argsort(feature_values)\n",
        "        feature_values, y = feature_values[sorted_indices], y[sorted_indices]\n",
        "        unique_thresholds = np.unique(feature_values)\n",
        "        quantiles = np.linspace(0, 1, num=self.quantiles + 2)[1:-1]  # Exclude 0 and 1\n",
        "        sampled_thresholds = np.quantile(unique_thresholds, quantiles)  # Get quantile values\n",
        "\n",
        "        mse_values = []\n",
        "        thresholds = []\n",
        "\n",
        "        for threshold in sampled_thresholds:\n",
        "            left_idxs = feature_values <= threshold\n",
        "            right_idxs = feature_values > threshold\n",
        "            if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:\n",
        "                continue\n",
        "\n",
        "            mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
        "            mse_values.append(mse)\n",
        "            thresholds.append(threshold)\n",
        "\n",
        "        return thresholds, mse_values\n",
        "\n",
        "    def _calculate_mse(self, left_y, right_y):\n",
        "        left_mse = np.var(left_y) * len(left_y) if len(left_y) > 0 else 0\n",
        "        right_mse = np.var(right_y) * len(right_y) if len(right_y) > 0 else 0\n",
        "        return (left_mse + right_mse) / (len(left_y) + len(right_y))\n",
        "\n",
        "    def _split(self, feature_column, threshold):\n",
        "        left_idxs = np.where(feature_column <= threshold)[0]\n",
        "        right_idxs = np.where(feature_column > threshold)[0]\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _calculate_leaf_value(self, y):\n",
        "\n",
        "        return 1 if np.mean(y) >= 0 else -1\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.traverse_tree(x, self.tree) for x in X])\n",
        "\n",
        "    def traverse_tree(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self.traverse_tree(x, node.left)\n",
        "        return self.traverse_tree(x, node.right)\n"
      ],
      "metadata": {
        "id": "f-A6NEq_elef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostingClassifierFromScratch:\n",
        "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=1, n_jobs=-1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.n_jobs = n_jobs  # Parallelization\n",
        "        self.models = []\n",
        "        self.init_pred = None\n",
        "\n",
        "    def _softmax(self, logits):\n",
        "        return softmax(logits, axis=1)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_classes = X.shape[0], len(np.unique(y))\n",
        "        self.init_pred = np.zeros((n_samples, n_classes))  # Initial prediction logits\n",
        "        pred = self.init_pred.copy()\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # print(f'Num of stums trained: {i}')\n",
        "            # Compute residuals (negative gradient of cross-entropy loss)\n",
        "            prob = self._softmax(pred)\n",
        "            residual = np.eye(n_classes)[y] - prob  # One-hot encoded y minus probabilities\n",
        "\n",
        "            # Train weak learners for each class in parallel\n",
        "            models = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(self._train_stump)(X, residual[:, class_idx])\n",
        "                for class_idx in range(n_classes)\n",
        "            )\n",
        "\n",
        "            # Update predictions\n",
        "            for class_idx, stump in enumerate(models):\n",
        "                pred[:, class_idx] += self.learning_rate * stump.predict(X)\n",
        "\n",
        "            self.models.append(models)\n",
        "            print(f\"Num of Trees Trained: {i + 1}\")\n",
        "\n",
        "    def _train_stump(self, X, residual):\n",
        "        stump = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "        stump.fit(X, residual)\n",
        "        return stump\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples, n_classes = X.shape[0], len(self.models[0])\n",
        "        pred = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for models in self.models:\n",
        "            for class_idx, stump in enumerate(models):\n",
        "                pred[:, class_idx] += self.learning_rate * stump.predict(X)\n",
        "\n",
        "        return np.argmax(self._softmax(pred), axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        n_samples, n_classes = X.shape[0], len(self.models[0])\n",
        "        pred = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for models in self.models:\n",
        "            for class_idx, stump in enumerate(models):\n",
        "                pred[:, class_idx] += self.learning_rate * stump.predict(X)\n",
        "\n",
        "        return self._softmax(pred)"
      ],
      "metadata": {
        "id": "AFsIDKMyFQKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    def __init__(self, number_of_components):\n",
        "        self.K = number_of_components\n",
        "        self.mean = None\n",
        "        self.components = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Center the data by subtracting the mean\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        X = X - self.mean\n",
        "\n",
        "        # Calculate the covariance matrix\n",
        "        cov = np.cov(X.T)#needs samples as columns\n",
        "        #eigenvectors and val\n",
        "        # Get eigenvalues and eigenvectors\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
        "\n",
        "        # Sort eigenvalues and eigenvectors in descending order\n",
        "        index = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[index]\n",
        "        eigenvectors = eigenvectors[:, index]\n",
        "\n",
        "        # Select the top K eigenvectors (principal components)\n",
        "        self.components = eigenvectors[:, :self.K]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Center the data\n",
        "        X = X - self.mean\n",
        "\n",
        "        # Project the data onto the top K components\n",
        "        return np.dot(X, self.components)\n"
      ],
      "metadata": {
        "id": "kvxUHr3IK3P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/DATASET'\n",
        "dataset = None\n",
        "class_names = []\n",
        "X_train, X_val, y_train, y_val, label_encoder = load_images(folder_path, image_size=(28, 28))\n",
        "for i in range(y_train.shape[0]):\n",
        "  if(y_train[i]!=0):\n",
        "    y_train[i]-=1\n",
        "print(np.unique(y_train))\n",
        "for i in range(data.shape[1]):\n",
        "  if(np.unique(data[:][i]).shape == 26):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seJOC01-K-SR",
        "outputId": "a35bfa82-1478-4b51-e0da-3b0eb0593c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from /content/drive/MyDrive/DATASET...\n",
            "Loaded 1355 images from 27 classes.\n",
            "Train set size: 1084, Validation set size: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/DATASET/A_Z_Handwritten_Data.csv'\n",
        "data = np.loadtxt(folder_path, delimiter=',')"
      ],
      "metadata": {
        "id": "BRQbcWhf3_fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_2d = data[100000][0:784].reshape(28, 28)\n",
        "plt.imshow(image_2d, cmap = 'gray')\n",
        "print(data[100000][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "gWVXccf26Bun",
        "outputId": "8813e876-bc0d-4927-bed9-62ab7702a547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-boo8XwCA0G-",
        "outputId": "43b9b5b9-f77f-4a19-8e1f-e57e045b01fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   4   5   7   8   9  10  12  14  19  20  22  25  27  28  30  33  34\n",
            "  36  37  49  54  57  64  66  67  69  74  78  80  81  91  94  96 100 103\n",
            " 105 109 118 119 122 123 126 128 129 133 138 139 140 141 143 147 153 166\n",
            " 167 168 176 179 185 187 189 190 196 197 199 203 206 209 213 214 215 217\n",
            " 219 220 223 225 227 228 229 230 231 233 235 236 241 242 243 244 245 246\n",
            " 248 249 251 252 253 254 255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(data)\n",
        "X_train = data[0:int(data.shape[0]*0.8)][1:]\n",
        "y_train = data[0:int(data.shape[0]*0.8)][0]\n",
        "y_train = y_train.astype(int)\n",
        "X_val = data[int(data.shape[0]*0.8):][1:]\n",
        "y_val = data[int(data.shape[0]*0.8):][0]\n",
        "y_val = y_val.astype(int)"
      ],
      "metadata": {
        "id": "l0v50GbG68Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce5K4BALFhGF",
        "outputId": "2fedc2cb-2416-477d-bf11-6ef051604428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(y_val.shape[0]):\n",
        "\n",
        "  if(y_val[i]!=0):\n",
        "    y_val[i]-=1\n"
      ],
      "metadata": {
        "id": "K139Z3AUIjdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yo = PCA(number_of_components = 50)\n",
        "yo.fit(X_train)\n",
        "X_pca_train = yo.transform(X_train)\n",
        "# yo.fit(X_val)\n",
        "X_pca_val = yo.transform(X_val)\n"
      ],
      "metadata": {
        "id": "MK28ulwcMSb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_pca_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWtY-50kMKoY",
        "outputId": "903c8db3-714a-4659-a9cc-b46b0e614fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(297959, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Train the Gradient Boosting Classifier\n",
        "    gbc = GradientBoostingClassifierFromScratch(n_estimators=50, learning_rate=0.1)\n",
        "    gbc.fit(X_pca_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYcKG1JwFfwz",
        "outputId": "6aa93342-e10f-4b88-f397-5bbae0b33b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of Trees Trained: 1\n",
            "Num of Trees Trained: 2\n",
            "Num of Trees Trained: 3\n",
            "Num of Trees Trained: 4\n",
            "Num of Trees Trained: 5\n",
            "Num of Trees Trained: 6\n",
            "Num of Trees Trained: 7\n",
            "Num of Trees Trained: 8\n",
            "Num of Trees Trained: 9\n",
            "Num of Trees Trained: 10\n",
            "Num of Trees Trained: 11\n",
            "Num of Trees Trained: 12\n",
            "Num of Trees Trained: 13\n",
            "Num of Trees Trained: 14\n",
            "Num of Trees Trained: 15\n",
            "Num of Trees Trained: 16\n",
            "Num of Trees Trained: 17\n",
            "Num of Trees Trained: 18\n",
            "Num of Trees Trained: 19\n",
            "Num of Trees Trained: 20\n",
            "Num of Trees Trained: 21\n",
            "Num of Trees Trained: 22\n",
            "Num of Trees Trained: 23\n",
            "Num of Trees Trained: 24\n",
            "Num of Trees Trained: 25\n",
            "Num of Trees Trained: 26\n",
            "Num of Trees Trained: 27\n",
            "Num of Trees Trained: 28\n",
            "Num of Trees Trained: 29\n",
            "Num of Trees Trained: 30\n",
            "Num of Trees Trained: 31\n",
            "Num of Trees Trained: 32\n",
            "Num of Trees Trained: 33\n",
            "Num of Trees Trained: 34\n",
            "Num of Trees Trained: 35\n",
            "Num of Trees Trained: 36\n",
            "Num of Trees Trained: 37\n",
            "Num of Trees Trained: 38\n",
            "Num of Trees Trained: 39\n",
            "Num of Trees Trained: 40\n",
            "Num of Trees Trained: 41\n",
            "Num of Trees Trained: 42\n",
            "Num of Trees Trained: 43\n",
            "Num of Trees Trained: 44\n",
            "Num of Trees Trained: 45\n",
            "Num of Trees Trained: 46\n",
            "Num of Trees Trained: 47\n",
            "Num of Trees Trained: 48\n",
            "Num of Trees Trained: 49\n",
            "Num of Trees Trained: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(gbc.init_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5jrDDNwQSRc",
        "outputId": "cac8fef5-2a71-476d-adbc-a7224c0e973d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHD3M51EF_wA",
        "outputId": "f175b4b3-c507-465d-8553-d792b5a757db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14  9 12 ...  1 16 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "y_pred = gbc.predict(X_pca_val)\n",
        "# y_test_labels = np. argmax(y_val, axis=1)\n",
        "\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Val Accuracy: {accuracy:.2f}\")\n",
        "y_pred = gbc.predict(X_pca_train)\n",
        "# y_test_labels = np. argmax(y_val, axis=1)\n",
        "accuracy = np.mean(y_pred == y_train)\n",
        "print(f\"train Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhRUh77uGi2e",
        "outputId": "d2b65260-3e2c-484c-c1ca-f63f3a4df60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting classes...\n",
            "Predicting probabilities...\n",
            "Val Accuracy: 0.49\n",
            "Predicting classes...\n",
            "Predicting probabilities...\n",
            "train Accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Bad implementation\n",
        "\n",
        "class GradientBoostingClassifier1:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.n_classes = None\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def to_one_hot(self, y):\n",
        "        n_samples = len(y)\n",
        "        one_hot = np.zeros((n_samples, self.n_classes))\n",
        "        one_hot[np.arange(n_samples), y] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def negative_gradient(self, y_true, y_pred):\n",
        "        return y_true - y_pred\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(f\"Starting training with {self.n_estimators} estimators...\")\n",
        "        self.n_classes = len(np.unique(y))\n",
        "        y_one_hot = self.to_one_hot(y)\n",
        "        F = np.zeros((X.shape[0], self.n_classes))\n",
        "\n",
        "        for m in range(self.n_estimators):\n",
        "            print(f\"Training estimator {m+1}/{self.n_estimators}...\")\n",
        "            p = self.softmax(F)\n",
        "            residuals = self.negative_gradient(y_one_hot, p)\n",
        "            trees_this_round = []\n",
        "\n",
        "            for k in range(self.n_classes):\n",
        "                print(f\"Fitting regression tree for class {k}...\")\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X, residuals[:, k])\n",
        "                F[:, k] += self.learning_rate * tree.predict(X)\n",
        "                trees_this_round.append((k, tree))\n",
        "\n",
        "            self.trees.append(trees_this_round)\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        print(\"Predicting probabilities...\")\n",
        "        F = np.zeros((X.shape[0], self.n_classes))\n",
        "        for trees_this_round in self.trees:\n",
        "            for k, tree in trees_this_round:\n",
        "                F[:, k] += self.learning_rate * tree.predict(X)\n",
        "        return self.softmax(F)\n",
        "\n",
        "    def predict(self, X):\n",
        "        print(\"Predicting classes...\")\n",
        "        probas = self.predict_proba(X)\n",
        "        return np.argmax(probas, axis=1)\n"
      ],
      "metadata": {
        "id": "BbnykWYLnrQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = GradientBoostingClassifier1(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
        "gbc.fit(X_pca_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUYhN7NEUQwx",
        "outputId": "4b00b220-b90f-4972-f518-8b31f3d5cdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with 10 estimators...\n",
            "Training estimator 1/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 2/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 3/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 4/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 5/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 6/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 7/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 8/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 9/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training estimator 10/10...\n",
            "Fitting regression tree for class 0...\n",
            "Fitting regression tree for class 1...\n",
            "Fitting regression tree for class 2...\n",
            "Fitting regression tree for class 3...\n",
            "Fitting regression tree for class 4...\n",
            "Fitting regression tree for class 5...\n",
            "Fitting regression tree for class 6...\n",
            "Fitting regression tree for class 7...\n",
            "Fitting regression tree for class 8...\n",
            "Fitting regression tree for class 9...\n",
            "Fitting regression tree for class 10...\n",
            "Fitting regression tree for class 11...\n",
            "Fitting regression tree for class 12...\n",
            "Fitting regression tree for class 13...\n",
            "Fitting regression tree for class 14...\n",
            "Fitting regression tree for class 15...\n",
            "Fitting regression tree for class 16...\n",
            "Fitting regression tree for class 17...\n",
            "Fitting regression tree for class 18...\n",
            "Fitting regression tree for class 19...\n",
            "Fitting regression tree for class 20...\n",
            "Fitting regression tree for class 21...\n",
            "Fitting regression tree for class 22...\n",
            "Fitting regression tree for class 23...\n",
            "Fitting regression tree for class 24...\n",
            "Fitting regression tree for class 25...\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gbc.predict(X_val)\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Val Accuracy: {accuracy:.2f}\")\n",
        "y_pred = gbc.predict(X_pca_train)\n",
        "accuracy = np.mean(y_pred == y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZTBRa7GHV0H",
        "outputId": "87d00bc0-3358-4cbc-a555-54fde7123bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting classes...\n",
            "Predicting probabilities...\n",
            "Val Accuracy: 0.05\n",
            "Predicting classes...\n",
            "Predicting probabilities...\n",
            "Train Accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.priors = None  # Class prior probabilities\n",
        "        self.likelihoods = None  # Likelihood for each feature given class\n",
        "        self.classes = None  # Unique class labels\n",
        "        self.feature_stats = None  # Mean and variance for features per class\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Naive Bayes model to the data.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        self.feature_stats = {cls: {'mean': [], 'var': []} for cls in self.classes}\n",
        "\n",
        "        for idx, cls in enumerate(self.classes):\n",
        "            # Get samples for the current class\n",
        "            X_cls = X[y == cls]\n",
        "\n",
        "            # Compute class prior\n",
        "            self.priors[idx] = len(X_cls) / n_samples\n",
        "\n",
        "            # Compute mean and variance for each feature\n",
        "            self.feature_stats[cls]['mean'] = X_cls.mean(axis=0)\n",
        "            self.feature_stats[cls]['var'] = X_cls.var(axis=0)\n",
        "\n",
        "    def _calculate_likelihood(self, x, cls):\n",
        "        \"\"\"\n",
        "        Calculate the likelihood of the data given the class using Gaussian distribution.\n",
        "        \"\"\"\n",
        "        mean = self.feature_stats[cls]['mean']\n",
        "        var = self.feature_stats[cls]['var']\n",
        "        # Gaussian PDF\n",
        "        likelihood = (1 / np.sqrt(2 * np.pi * var)) * np.exp(-0.5 * ((x - mean) ** 2 / var))\n",
        "        return np.log(likelihood + 1e-9)  # Avoid log(0) by adding a small constant\n",
        "\n",
        "    def _calculate_posterior(self, x):\n",
        "        \"\"\"\n",
        "        Calculate the posterior probability for each class given the data point.\n",
        "        \"\"\"\n",
        "        posteriors = []\n",
        "\n",
        "        for idx, cls in enumerate(self.classes):\n",
        "            # Start with the log of the prior\n",
        "            prior = np.log(self.priors[idx])\n",
        "\n",
        "            # Add the log likelihood\n",
        "            likelihood = self._calculate_likelihood(x, cls).sum()\n",
        "            posterior = prior + likelihood\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        return self.classes[np.argmax(posteriors)]  # Return class with highest posterior\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the input data.\n",
        "        \"\"\"\n",
        "        return np.array([self._calculate_posterior(x) for x in X])"
      ],
      "metadata": {
        "id": "X8EaCQjqoQJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayesClassifier()\n",
        "nb.fit(X_pca_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cc3fts4ar4Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "y_pred = nb.predict(X_pca_val)\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Val Accuracy: {accuracy:.2f}\")\n",
        "y_pred = nb.predict(X_pca_train)\n",
        "accuracy = np.mean(y_pred == y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8dlrSh5sHI0",
        "outputId": "8507bda3-3ece-44ba-b725-0aa61a4afd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 0.06\n",
            "Train Accuracy: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxOypYWCG6FR",
        "outputId": "be42dccb-368e-4466-fc58-1eef278c1b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5 12  6 14  4  7  4  3  4 16 25  3 15 14  4  3 13 12 14 14 13 14 13 12\n",
            "  7 14  0  7 14 13  3 12 13 12 15 12 13 12 14 14  7 12  4 10  3 14  5  9\n",
            " 21  0 14 12 12 17  5 14  4 12  7  4 14  0  6 14  0  4 14  7 17 13  6 12\n",
            "  4 13  7 14 14 12 14 12  0 14 21 14 14  4 13  7  4 12 12 14 17  4  0  0\n",
            " 14  0 14 14 17 10 16 14  0 13 14  4 12  3 14  5 15 14  4 12 17  7  7  0\n",
            "  5  7 12  3 10 14  1 12  7  7  4 14 14 15  4  1  4  4 14 14 14  5 12 15\n",
            "  5 14  4  4 14 21  9 14  4 21  6  5 14 15 12 14 14  4 14  4  4 21  4 12\n",
            "  0  4  0 12 14  4  5 12 14 12 21  6 12 21  4 15 21 12 13  4  9 14 14 12\n",
            " 12 12 21 16  8  4 14 14 14 14  4 11  0  6 14 16 13 10 16 14 12 17 12  4\n",
            " 12 10 12  1 12  0 15  0 12  2  0  3  5 12 14 15  4  4  4 12 13 12 14  0\n",
            " 12 14 14 21 10 17 14 12 10 10 13  0 14 12  5  4 14  9 14 17  7 12 10 12\n",
            "  3 12  4  8 12 14 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5x3Gy9EG284",
        "outputId": "75fe7b7b-fb15-4de8-9edf-9f24f01c2112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 14  8 17 18  2 23  0 23 11 10  8  4 13 24 11  3  8 21 15 13  1 17 14\n",
            " 10  0  6  5 14  8 21 12  8 22 18 14  3  0  4 16 20  0 23 15 16 25  5  5\n",
            " 14 13 21 24 12  6  5  1  4  0  5  3  3 16  6  6  7 14  4  2 16  8 14 16\n",
            " 15  6 15  6  1 25  7 12  7 17 13 15  4 15  3 21 24  4 12 12  4  7  7  3\n",
            "  1  7 11  5  2 15  4  1 24 13  1 10  0  8 21  9  1  7  3  4 19 10  0 12\n",
            "  7  0 13 13 15  9 10 21 10 10 15  6  6 10  3  4  6  8 11  7 16  9  3 20\n",
            "  5 16  8  7 12  0  4 14 11 12 17  9 15 18 12 20 15 14 13  9 17  4 14  6\n",
            " 16 25 14 24  6 17  7  7 12 20 14  0 21  2 16 20  0 25  3  4  5 21 11 14\n",
            " 11 20  1 15  8  9 12 16  1  4 14 13 14 18  9 21  3 15  2  9 21  4 20 23\n",
            " 12 15 16 14 13 14  7 14 22 17 17  0  5 21 14 10  5 17 11 20  6 19 12 17\n",
            " 22 11 14 11  7 19 16  1 17  7 13  0 19 22  5 15  9  5 14 19 10 17  8  4\n",
            "  8 25 23  8 22  6 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HOCuJiwPHBLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}