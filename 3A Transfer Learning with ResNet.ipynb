{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx7PDH1BfVs2",
        "outputId": "23d44f1e-c813-4d27-eca2-31d7706a2e13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wnxLLyEIZ1bx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, RobertaTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from transformers import VisionEncoderDecoderModel\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import ViTFeatureExtractor, RobertaTokenizer\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths (update to your dataset structure in Drive)\n",
        "base_path = '/content/drive/My Drive/HandwritingDataset/'  # Change to your folder structure\n",
        "train_csv = os.path.join(base_path, 'train_v2.csv')\n",
        "val_csv = os.path.join(base_path, 'val_v2.csv')\n",
        "test_csv = os.path.join(base_path, 'test_v2.csv')\n",
        "train_dir = os.path.join(base_path, 'train_v2/train')\n",
        "val_dir = os.path.join(base_path, 'val_v2/val')\n",
        "test_dir = os.path.join(base_path, 'test_v2/test')\n",
        "\n",
        "# Feature extractor and tokenizer\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "\n",
        "# Dataset class\n",
        "class HandwritingDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, feature_extractor, tokenizer, max_target_length=128, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: Pandas DataFrame containing filenames and labels.\n",
        "            img_dir: Path to the directory containing images.\n",
        "            feature_extractor: ViTFeatureExtractor instance for image preprocessing.\n",
        "            tokenizer: RobertaTokenizer instance for label tokenization.\n",
        "            max_target_length: Maximum length of tokenized labels.\n",
        "            transform: Additional transformations for images (optional).\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.img_dir = img_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_target_length = max_target_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['FILENAME'])\n",
        "        label = row['IDENTITY']\n",
        "\n",
        "        # Open image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Image not found: {img_path}\")\n",
        "            raise\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract features\n",
        "        pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        # Tokenize label\n",
        "        labels = self.tokenizer(\n",
        "            label,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_target_length,\n",
        "            truncation=True\n",
        "        ).input_ids\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
      ],
      "metadata": {
        "id": "QLj2AUTNeT81"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Prepare datasets and dataloaders\n",
        "train_dataset = HandwritingDataset(train_data, train_dir, feature_extractor, tokenizer, transform=transform)\n",
        "val_dataset = HandwritingDataset(val_data, val_dir, feature_extractor, tokenizer, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "vWxufau3lVNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained TrOCR model\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 10\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Define training function\n",
        "def train_model(model, train_loader, val_loader, optimizer, lr_scheduler, num_epochs=10):\n",
        "    model.train()\n",
        "    device = \"cuda\"\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        print(f\"Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        validate_model(model, val_loader)\n",
        "\n",
        "# Validation function\n",
        "def validate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    device = \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")"
      ],
      "metadata": {
        "id": "un-wpnj7f-31"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(pixel_values=images, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(pixel_values=images, labels=labels)\n",
        "                val_loss += outputs.loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "e1v0hFJbfpvl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, optimizer, lr_scheduler, num_epochs=10)"
      ],
      "metadata": {
        "id": "90yTWZnogJxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "    device = \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model.generate(pixel_values=pixel_values)\n",
        "\n",
        "            # Decode predictions and references\n",
        "            pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            predictions.extend(pred_texts)\n",
        "            references.extend(ref_texts)\n",
        "\n",
        "    return predictions, references"
      ],
      "metadata": {
        "id": "A5nyBoDclqaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1(predictions, references):\n",
        "    char_f1_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        char_f1_scores.append(f1_score(list(ref), list(pred), average=\"weighted\"))\n",
        "    return sum(char_f1_scores) / len(char_f1_scores)\n",
        "\n",
        "predictions, references = test_model(model, test_loader)\n",
        "f1 = calculate_f1(predictions, references)\n",
        "print(f\"Character-wise F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "ABSDXDMJls1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    from transformers import ViTFeatureExtractor, RobertaTokenizer\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    from transformers import VisionEncoderDecoderModel, AdamW, get_scheduler\n",
        "\n",
        "    # Paths to data\n",
        "    train_csv = 'train_v2.csv'\n",
        "    val_csv = 'val_v2.csv'\n",
        "    test_csv = 'test_v2.csv'\n",
        "    train_dir = 'train_v2/train'\n",
        "    val_dir = 'val_v2/val'\n",
        "    test_dir = 'test_v2/test'\n",
        "\n",
        "    # Load feature extractor and tokenizer\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "\n",
        "    # Define image transformations\n",
        "    from torchvision import transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Prepare datasets and dataloaders\n",
        "    train_dataset = HandwritingDataset(pd.read_csv(train_csv), train_dir, feature_extractor, tokenizer, transform=transform)\n",
        "    val_dataset = HandwritingDataset(pd.read_csv(val_csv), val_dir, feature_extractor, tokenizer, transform=transform)\n",
        "    test_dataset = HandwritingDataset(pd.read_csv(test_csv), test_dir, feature_extractor, tokenizer, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "    # Initialize model\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    # Modify the decoder's vocabulary size to match your dataset (words or characters)\n",
        "    model.config.decoder.vocab_size = tokenizer.vocab_size\n",
        "    model.decoder.resize_token_embeddings(tokenizer.vocab_size)\n",
        "\n",
        "    # Define optimizer and learning rate scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    num_training_steps = len(train_loader) * 10  # Assuming 10 epochs\n",
        "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "    # Train and validate the model\n",
        "    train_model(model, train_loader, val_loader, optimizer, lr_scheduler, num_epochs=10)\n",
        "\n",
        "    # Test the model and evaluate F1 score\n",
        "    predictions, references = test_model(model, test_loader)\n",
        "    f1 = calculate_f1(predictions, references)\n",
        "    print(f\"Character-wise F1 Score on Test Data: {f1:.4f}\")\n",
        "\n",
        "    # Save predictions and model\n",
        "    results = pd.DataFrame({\"Predicted\": predictions, \"Reference\": references})\n",
        "    results.to_csv(\"test_results.csv\", index=False)\n",
        "\n",
        "    model.save_pretrained(\"fine_tuned_trocr_model\")\n",
        "    tokenizer.save_pretrained(\"fine_tuned_trocr_model\")\n"
      ],
      "metadata": {
        "id": "d-FP1bd7mK4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}